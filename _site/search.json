[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Documenting my home server setup\n\n\n\n\n\n\n\nlinux\n\n\nservers\n\n\nprocess write-up\n\n\n\n\nThe steps I followed to configure a HP ProDesk 600 G5 Mini as a modest Linux server for small data science projects.\n\n\n\n\n\n\nNov 11, 2023\n\n\nJacob Eliason\n\n\n\n\n\n\n  \n\n\n\n\nPrivacy-friendly web analytics for Quarto\n\n\n\n\n\n\n\nquarto\n\n\nmeta\n\n\n\n\nConfiguring GoatCounter on my personal site.\n\n\n\n\n\n\nNov 6, 2023\n\n\nJacob Eliason\n\n\n\n\n\n\n  \n\n\n\n\nSetting up TensorFlow with GPU support on a M1 Macbook Pro\n\n\n\n\n\n\n\ndeep learning\n\n\npython\n\n\nprocess write-up\n\n\n\n\nFor training small neural networks locally.\n\n\n\n\n\n\nMay 8, 2023\n\n\nJacob Eliason\n\n\n\n\n\n\n  \n\n\n\n\nWriting an R wrapper for a prediction market’s API\n\n\n\n\n\n\n\nr\n\n\nr packages\n\n\nforecasting\n\n\n\n\nHow to “bet on anything with play money” from your console\n\n\n\n\n\n\nNov 27, 2022\n\n\nJacob Eliason\n\n\n\n\n\n\n  \n\n\n\n\nDeploying a Twitter bot from RStudio\n\n\n\n\n\n\n\nr\n\n\ntwitter\n\n\ngithub actions\n\n\n\n\nInto the mapbotverse.\n\n\n\n\n\n\nNov 11, 2021\n\n\nJacob Eliason\n\n\n\n\n\n\n  \n\n\n\n\nRe-weighting to estimate population characteristics within a subset of a survey sample\n\n\n\n\n\n\n\nsurveys\n\n\ndemography\n\n\n\n\nExploring a surprising claim about demographic change.\n\n\n\n\n\n\nJun 25, 2021\n\n\nJacob Eliason\n\n\n\n\n\n\n  \n\n\n\n\nPreparing data from Strava for analysis\n\n\n\n\n\n\n\nr\n\n\nsports\n\n\nprocess write-up\n\n\n\n\nHow I clean my workout data.\n\n\n\n\n\n\nApr 30, 2021\n\n\nJacob Eliason\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob Eliason",
    "section": "",
    "text": "I am a data scientist currently working towards a master’s degree in statistics at the London School of Economics.\nI have experience in statistical modeling, deep learning, and survey research. I like working on hard problems.\nI also enjoy running, hiking, and film photography."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Jacob Eliason",
    "section": "",
    "text": "I am a data scientist currently working towards a master’s degree in statistics at the London School of Economics.\nI have experience in statistical modeling, deep learning, and survey research. I like working on hard problems.\nI also enjoy running, hiking, and film photography."
  },
  {
    "objectID": "posts/2021-04-30-processing-data-from-strava/index.html",
    "href": "posts/2021-04-30-processing-data-from-strava/index.html",
    "title": "Preparing data from Strava for analysis",
    "section": "",
    "text": "When I run, I use Strava to log my activity. In honor of recently running my one-thousandth mile on Strava, I thought I’d do a write up for the steps I use to process my user data in R. The data Strava makes available is granular and can be used for all kinds of fun things after the steps detailed here."
  },
  {
    "objectID": "posts/2021-04-30-processing-data-from-strava/index.html#s1",
    "href": "posts/2021-04-30-processing-data-from-strava/index.html#s1",
    "title": "Preparing data from Strava for analysis",
    "section": "1. Export your data",
    "text": "1. Export your data\nPer the instructions on their website, you can export your Strava activity data by navigating to your profile in a web browser and following Settings &gt; My Account &gt; Download or Delete your Account - Get Started &gt; Request Your Archive. From that point, it takes me about 10 minutes to see a download link in my inbox.\nThe download apparently includes a lot of different kinds of data but the most salient (for my account, anyway) are contained in activities.csv and the activities/ directory. The former contains summary information for each of my Strava activities and the latter contains individual files, each of which have second-to-second position data for an individual run, hike, or bike ride. The activity files appear to be some kind of custom or proprietary exercise file type–the two extensions I notice are .gpx and .fit.gz. At first glance, I don’t recognize either.\nFortunately, as usual I find that someone else has already done the heavy lifting for the most important part of this process. The Github packages FITfileR and trackeR can be used to convert these file types into something more legible. Special thanks to Mike Smith for his excellent work on the former."
  },
  {
    "objectID": "posts/2021-04-30-processing-data-from-strava/index.html#s2",
    "href": "posts/2021-04-30-processing-data-from-strava/index.html#s2",
    "title": "Preparing data from Strava for analysis",
    "section": "2. Unpacking .gpx and .fit.gz files",
    "text": "2. Unpacking .gpx and .fit.gz files\nI start by installing the Github packages and loading those along with the tidyverse.\n\n# devtools::install_github(\"grimbough/FITfileR\")\n# devtools::install_github(\"trackerproject/trackeR\")\n\nlibrary(FITfileR)\nlibrary(trackeR)\nlibrary(tidyverse)\n\nA few more lines help with setup and prepare for reading the activity files.\n\nPATH &lt;- str_c(str_remove(getwd(),\"/jacobeliason.com/posts/2021-04-30-processing-data-from-strava\"),\"/personal-projects/strava\")\nexport_date &lt;- \"2021-04-29\"\nPATH_ACTIVITIES &lt;- str_c(PATH, \"/DATA/\",export_date,\"/activities/\")\nactivity_names &lt;- list.files(PATH_ACTIVITIES)\nsample(activity_names, 3) # check to make sure I got the correct file path\n\nAs I look at the file names, the first thing that becomes apparent is that I have some extra work to do as a result of my alternately using my phone and a Garmin watch to record activities. Those two devices produce the two different file extensions I observe and require different steps for unpacking.\n\nUncompressing and reading files from my fitness watch (.fit.gz)\nThe .fit.gz files are compressed and need to be uncompressed to .fit before I can use the FITfileR package.\n\ncompressed_record_names &lt;- activity_names[str_sub(activity_names,-6,-1) == \"fit.gz\"]\n\nfor(i in 1:length(compressed_record_names)){\n  R.utils::gunzip(\n    str_c(PATH_ACTIVITIES, compressed_record_names[i]),\n    remove = F\n  )\n}\n\nHaving unzipped the files, I again collect names.\n\nactivity_names &lt;- list.files(PATH_ACTIVITIES)\nuncompressed_fit_names &lt;- activity_names[str_sub(activity_names,-3,-1) == \"fit\"] # want exact match to .fit only, no .fit.gz\n\nNow, using FITfileR::records(), I transform the files into tidy, rectangular datasets.\n\nlist.fit &lt;- list()\nfor(i in 1:length(uncompressed_fit_names)) {\n  record &lt;- FITfileR::readFitFile(\n    str_c(PATH_ACTIVITIES, uncompressed_record_names[i])\n  ) %&gt;% FITfileR::records()\n  \n  if(length(record) &gt; 1) {\n    record &lt;- record %&gt;% bind_rows() %&gt;% mutate(activity_id = i, filename = uncompressed_fit_names[i])\n  }\n  \n  list.fit[[i]] &lt;- record\n}\nfit_records &lt;- list.fit %&gt;% bind_rows() %&gt;% arrange(timestamp)\n\n\n\nReading files recorded from my iPhone (.gpx)\nI turn my attention back to the .gpx files. Fortunately, these files don’t require much beyond a simple pass from the trackeR function. I do some additional housekeeping along the way, but this part is pretty straightforward.\n\ngpx_names &lt;- activity_names[str_sub(activity_names,-3,-1) == \"gpx\"]\n\nlist.gpx &lt;- list()\nfor(i in 1:length(gpx_names)) {\n  record &lt;- trackeR::readGPX(str_c(PATH_ACTIVITIES, gpx_names[i])) %&gt;% \n    as_tibble() %&gt;% \n    rename(\n      timestamp = time, \n      position_lat = latitude, \n      position_long = longitude, \n      cadence = cadence_running\n    )\n  list.gpx[[i]] &lt;- record\n}\n\n\n\nCombine both record types\nI add my two datasets together and with that, I’m ready to Learn Things.\n\nrecords &lt;- bind_rows(\n  fit_records,\n  list.gpx %&gt;% bind_rows()\n) %&gt;% arrange(timestamp)\n\n# colnames(records)\n# nrow(records)\n\n\n\nStraightening out the summary information in activities.csv\nOne last thing I’ll do before I finish up is make some tweaks to the activities.csv file I got in my original download. I make some changes to the column names and order to taste, and I remove rows with empty file names. It turns out that those correspond with activities with no associated GPS data, such as treadmill or weightlifting workouts.\n\nrecord_key_raw &lt;- \n  activities %&gt;% \n  janitor::clean_names() %&gt;% # helper function for column names\n  janitor::remove_empty() %&gt;% # drop empty rows\n  select(filename, everything()) %&gt;% # reorder columns\n  filter(!is.na(filename)) # drop rows with empty file names\n\nI also make a variety of mostly trivial changes for my own convenience and then I’m good to go!\n\nKM_TO_MI &lt;- 0.621371\nM_TO_FT &lt;- 3.28084\n\nrecord_key &lt;- record_key_raw %&gt;% \n  \n  # change units for elevation variables\n  mutate_at(vars(contains(\"elevation\")), function(x){x &lt;- x*M_TO_FT}) %&gt;% \n  mutate(\n    \n  # units #\n    distance = distance*KM_TO_MI,\n    duration = elapsed_time/60,\n    duration_moving = moving_time/60,\n    pace = (duration/distance) %&gt;% round(2),\n    pace_moving = (duration_moving/distance) %&gt;% round(2),\n    \n  # ids #\n    filename = filename %&gt;% str_remove(., \"activities/\") %&gt;% str_replace(., \"fit.gz\", \"fit\"),\n    activity_id = as.character(activity_id),\n    activity_type = tolower(activity_type),\n    \n  # incorrectly coded activities #\n    activity_type = ifelse(filename == \"1812636545.gpx\", \"hike\", activity_type), \n    activity_type = ifelse(filename == \"3324264305.fit\", \"walk\", activity_type), \n    \n    \n  # dates #\n    rdatetime_utc = lubridate::as_datetime(activity_date, format = \"%b %d, %Y, %I:%M:%S %p\", tz = \"UTC\"),\n    rdatetime_et = lubridate::as_datetime(rdatetime_utc, tz = \"America/New_York\"),\n    rdate_et = lubridate::as_date(rdatetime_et), \n    \n    rday = lubridate::day(rdate_et),\n    rmonth = lubridate::month(rdate_et),\n    ryear = lubridate::year(rdate_et),\n    rhour_et = lubridate::hour(rdatetime_et),\n    rminute_et = lubridate::minute(rdatetime_et)\n\n  ) %&gt;% \n  select( # drop empty variables\n    -contains(\"weather\"), -contains(\"precipitation\"), -contains(\"wind\"),\n    -apparent_temperature, -sunrise_time, -sunset_time, -dewpoint, -humidity, -cloud_cover, -uv_index\n  ) %&gt;% \n  mutate_if(is.numeric, ~round(.x, 2)) # round numeric variables\n\nNow, for each run, I have information on granular location data and summary information in datasets records and record_key respectively. The interesting stuff pretty much all comes after this point, but I’ll save that for another post."
  },
  {
    "objectID": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html",
    "href": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html",
    "title": "Re-weighting to estimate population characteristics within a subset of a survey sample",
    "section": "",
    "text": "On Monday, Jana Riess (who I’ve admired for some time since reading her fascinating book) published a surprising finding in Religious News Service showing that 18% of Gen Z Mormons are lesbian, gay, or bisexual.\nJana’s findings were picked up by several local and national news sources, including the Salt Lake Tribune and the Washington Post. They also generated some lively discussion on social media.\nToday, she posted an update explaining how the true number was probably lower than 18% due to a misunderstanding regarding the Nationscape project’s weighting scheme.\nAfter exploring the Nationscape dataset, I believe we can produce a better estimate for that true number by applying a new weighting scheme to just the Mormon respondents in the Nationscape study."
  },
  {
    "objectID": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#context",
    "href": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#context",
    "title": "Re-weighting to estimate population characteristics within a subset of a survey sample",
    "section": "",
    "text": "On Monday, Jana Riess (who I’ve admired for some time since reading her fascinating book) published a surprising finding in Religious News Service showing that 18% of Gen Z Mormons are lesbian, gay, or bisexual.\nJana’s findings were picked up by several local and national news sources, including the Salt Lake Tribune and the Washington Post. They also generated some lively discussion on social media.\nToday, she posted an update explaining how the true number was probably lower than 18% due to a misunderstanding regarding the Nationscape project’s weighting scheme.\nAfter exploring the Nationscape dataset, I believe we can produce a better estimate for that true number by applying a new weighting scheme to just the Mormon respondents in the Nationscape study."
  },
  {
    "objectID": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#the-problem",
    "href": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#the-problem",
    "title": "Re-weighting to estimate population characteristics within a subset of a survey sample",
    "section": "The problem",
    "text": "The problem\nAccording to the Nationscape Representativeness Assessment (included with data download), survey responses are weighted to be representative of the US population. The assessment demonstrates through various comparisons to external sources that “the methodology employed in Nationscape generates estimates of general population characteristics [emphasis added] that are…closely aligned with government survey benchmarks.”\nAs Jana explains in the update to her original post, the problem is that while the weighted responses from the Nationscape dataset as a whole do a reasonably good job at describing the “general population,” there’s no reason to expect that a subset of the data will do the same for a corresponding smaller population.\nIn this case, the subset of the data is the 3,881 respondents (about 1% of the total) who selected “Mormon” in response to the question, “What is your present religion, if any?” Based on these respondents, we’d like to make inferences about the population of all adults in the United States who describe themselves as Mormon. However, these 3,881 individuals are different from that population of interest in some important ways. How do we know that? We don’t know precisely, since the Mormon church doesn’t publish exact figures for its membership, but there exist some resources that can help paint an approximate picture. Pew Research, in particular, has produced a number of resources for Mormon population demographics that seem to me a reasonable baseline for comparison (as I’ll mention later, this is among the decisions I’d be happy to replace with something better-informed.)\nTo give a couple of examples: Pew’s Religious Landscape Study estimated that in 2014, 70% of US Mormons identified as Republican or leaned Republican. In the Nationscape dataset, only 60% of Mormons identified as Republican or leaned Republican. In 2009, Pew estimated using Census-defined regions that 76% of US Mormons lived in the West—however, in fact only 64% of Nationscape Mormons were from the West.\nThe problem is if Republican Mormons or Mormons from the West are likely to answer the question of interest differently than other Mormons. If we don’t do anything about that, we’re not going to get a very good estimate of the true population figure."
  },
  {
    "objectID": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#proposed-solution",
    "href": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#proposed-solution",
    "title": "Re-weighting to estimate population characteristics within a subset of a survey sample",
    "section": "Proposed solution",
    "text": "Proposed solution\nThe good news is this: the Nationscape sample (like nearly every nationwide survey project) is different from its target population too, and their solution to that problem is pretty accessible. From the Representativeness Assessment:\n\nThe survey data are…weighted to be representative of the American population. Our weights are generated using a simple raking technique, as there is little benefit to more complicated approaches (Mercer et al. 2018). One set of weights is generated for each week’s survey. The targets to which Nationscape is weighted are derived from the adult population of the 2017 American Community Survey of the U.S. Census Bureau. The one exception is the 2016 vote, which is derived from the official election results released by the Federal Election Commission.\n\nOn reading that, I wondered if there was any reason a new weighting scheme couldn’t be devised for application to the Mormon subset alone. All we’d need are new targets that describe characteristics of, instead of the adult population of the US, the adult population of Mormons in the US.\nI picked some variables that seemed relevant and had reasonable population estimates readily available.\n\nCensus region (Pew, 2009)\nAge (Pew RLS, 2014)\nGender (Pew RLS, 2014)\nRace (Pew RLS, 2014)\nEducation (Pew RLS, 2014)\nParty ID (Pew RLS, 2014)\n\nIf anybody reading this has thoughts about what variables or sources might be better than those shown here, please drop a comment below. All of these—Region in particular—are somewhat dated and I’m actually quite sure better sources exist. I tried to limit myself with how long I spent on this post because I wanted to get feedback before too much time passes.\nHaving compiled population targets from the sources given above, I show how both the unweighted estimates and the estimates produced using the original Nationscape weights fail to accurately represent the adult population of US Mormons.\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nLevel\nUnweighted estimate\nPopulation target\nWith original weights\n\n\n\n\nAge\n18-29\n26%\n22%\n26%\n\n\nAge\n30-49\n40%\n40%\n37%\n\n\nAge\n50-64\n21%\n22%\n22%\n\n\nAge\n65+\n12%\n16%\n16%\n\n\nRegion\nNortheast\n6%\n4%\n5%\n\n\nRegion\nMidwest\n10%\n7%\n9%\n\n\nRegion\nSouth\n20%\n12%\n21%\n\n\nRegion\nWest\n64%\n76%\n65%\n\n\nEducation\nHigh school or less\n22%\n27%\n34%\n\n\nEducation\nSome college\n42%\n40%\n41%\n\n\nEducation\nCollege grad\n22%\n23%\n15%\n\n\nEducation\nPost grad\n14%\n10%\n10%\n\n\nGender\nFemale\n51%\n56%\n52%\n\n\nGender\nMale\n49%\n44%\n48%\n\n\nParty\nDemocrat\n30%\n19%\n30%\n\n\nParty\nRepublican\n60%\n70%\n59%\n\n\nParty\nIndependent\n10%\n11%\n10%\n\n\nRace\nWhite\n82%\n85%\n79%\n\n\nRace\nNon-white\n18%\n15%\n21%\n\n\n\n\n\nSome of the differences between the unweighted estimates and the population targets are within the margin of error or simply unimportant to the question at hand. However, if nothing else there’s clear evidence that applying the original Nationscape weights provides no systematic improvement to the unweighted estimates in terms of what we understand “Mormons in the United States” to really look like. Both the unweighted estimates and the estimates produced using the original Nationscape weights will almost certainly generate estimates for Mormonism that are, for example, too Southern, too Democrat, and too male.\nWe can produce much more accurate estimates by using new weights for the Mormon respondents that are based on these population targets. Using a “simple raking technique” (in my case, using the survey package in R), I bring the sample characteristics of the Mormon survey respondents more or less into alignment with what we understand the population characteristics of Mormons to really be. Here’s what the new weighted estimates and the population targets look like side by side.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nLevel\nWith new weights\nPopulation target\n\n\n\n\nAge\n18-29\n22%\n22%\n\n\nAge\n30-49\n40%\n40%\n\n\nAge\n50-64\n22%\n22%\n\n\nAge\n65+\n16%\n16%\n\n\nRegion\nNortheast\n4%\n4%\n\n\nRegion\nMidwest\n7%\n7%\n\n\nRegion\nSouth\n12%\n12%\n\n\nRegion\nWest\n77%\n76%\n\n\nEducation\nHigh school or less\n27%\n27%\n\n\nEducation\nSome college\n40%\n40%\n\n\nEducation\nCollege grad\n23%\n23%\n\n\nEducation\nPost grad\n10%\n10%\n\n\nGender\nFemale\n56%\n56%\n\n\nGender\nMale\n44%\n44%\n\n\nParty\nDemocrat\n19%\n19%\n\n\nParty\nRepublican\n70%\n70%\n\n\nParty\nIndependent\n11%\n11%\n\n\nRace\nWhite\n85%\n85%\n\n\nRace\nNon-white\n15%\n15%\n\n\n\n\n\nThese obviously are a much closer match. Going one step further, I highlight the “performance” of these different schemes by taking the difference between each set of estimates and the population targets. Using those differences, I compute the root-mean-square error for each scheme overall (where a higher number is more error and “worse”).1\n\n\n\n\n\nMethod\nRMSE\n\n\n\n\nNew weights\n0.0020\n\n\nNationscape weights\n0.0600\n\n\nUnweighted\n0.0541\n\n\n\n\n\nInterestingly, the Nationscape weights actually make Mormon estimates less like the Mormon population targets than using no weights at all would."
  },
  {
    "objectID": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#new-results",
    "href": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#new-results",
    "title": "Re-weighting to estimate population characteristics within a subset of a survey sample",
    "section": "New results",
    "text": "New results\nThat said, what difference do these new weights make with respect to our question at hand? Using the new weights on the sample of 3,881 Mormons, I show the following results for “Do you identify as [straight/gay/lesbian/bisexual/other]” by generation.\n\n\n\n\n\nGeneration\nStraight\nLGB\nOther\nPrefer not to say\nRefused\n\n\n\n\nBoomer+Silent\n95%\n2%\n0%\n2%\n0%\n\n\nGen X\n93%\n5%\n1%\n1%\n0%\n\n\nMillennial\n86%\n11%\n1%\n2%\n0%\n\n\nGen Z\n82%\n14%\n1%\n2%\n0%\n\n\n\n\n\nFurthermore, I produce confidence intervals, incorporating the adjustment recommended in the Nationscape User Guide:\n\nResearchers should take care in calculating margins of error or standard errors due to the non-random nature of the sample. Standard calculations based on random sampling will underestimate the true magnitude of random error in our sample . As a starting place, we recommend the adjustment proposed by Rivers and Bailey (2009).\n\nRivers and Bailey give the following to estimate variance:\n\\(V(\\hat{\\theta}) = (1 + {s^2}_{w})\\hat{\\theta}(1-\\hat{\\theta})\\) where \\(s_w\\) is the standard deviation of the weights\nMy interpretation of Rivers and Bailey follows code like this. I include it here because I’d love to hear feedback if this is a misunderstanding of that recommendation.\n\nsd_w_mormon &lt;- sd(responses_mormon$weight_mormon)\nvif_mormon &lt;- (1+sd_w_mormon^2)\nlower_95 &lt;- est - (qnorm(0.975) * sqrt(vif_mormon*(est*(1-est))/wt_n))\nupper_95 &lt;- est + (qnorm(0.975) * sqrt(vif_mormon*(est*(1-est))/wt_n))\n\nUsing these confidence intervals (and corresponding calculations for the general population), I show the following final result:\n\n\n\nimage\n\n\nThe new estimate for LGB-identifying US Gen Z Mormons is 14% (95% CI 10.4%, 18.2%)."
  },
  {
    "objectID": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#discussion",
    "href": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#discussion",
    "title": "Re-weighting to estimate population characteristics within a subset of a survey sample",
    "section": "Discussion",
    "text": "Discussion\nThis estimate is 4 percentage points lower than the original estimate of 18%. The “LGB+Other” estimate is 15%, which is 7 percentage percentage points lower than the original estimate of 22%. This seems to be somewhat less of a difference than asserted in Jana’s update to her original post, where she predicted that the true value is “around 7 to 9 percentage points lower [than was found using the default Nationscape weights].” I’d be interested to know what methods informed her updated prediction. I know I’m not necessarily aware of what external considerations may be necessary for this analysis beyond what’s specified in the documentation for this dataset.\nHowever, my estimate sounds like it’s at least in the ballpark—and in any case, we’re talking about pretty large standard errors here. I’m also again very open to lots of potential improvements to this analysis, particularly in regards to the selection of population targets. If I had more time, I’d especially get further input in the decisions about Party ID2, for which rates have famously changed in interesting ways in the past decade and during the era of Donald Trump among American Mormons in particular. I’m further interested in better describing what kinds of bias remain after implementing this kind of weighting process. Even with the recommended addition of the variance inflation factor (and with intervals as large as they are), I can’t help but feel that I’m still underestimating the standard errors, that there’s more variability than these standard calculations capture. That, however, is also more broadly true to some extent across the survey research industry.3\nOne thing that gives me some confidence, though, is a certain piece of research that seems to corroborate this new, lower estimate. In 2018, researchers sent a survey to all 30,840 undergraduates at Brigham Young University to learn about “religiosity, mental health outcomes, and sexual minority identity” among the (99% Mormon) student body. The survey was university-approved and resulted in an incredible 24% response rate. The researchers estimated that the proportion of undergraduates who identified as something other than “exclusively heterosexual” at 13.1%.\nBecause I’m trying to wrap this up, I’m not going to get into whether that paper necessarily represents an accurate estimate of queer BYU students.4 It does, however, appear to show prevalence in the neighborhood of what the new weights for Nationscape find, and that feels encouraging.\nIn conclusion, I’d describe this estimate as flawed but improved. I think it better accounts for some variability due to non-response, but probably still fails to account for variability in other ways! My code for the new weighting scheme can be found along with the code for this post on my Github. If anyone has any feedback or ideas for improvement about anything I’ve done here, I’d love if you’d leave a comment or contact me directly at jacobeliason at gmail dot com."
  },
  {
    "objectID": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#footnotes",
    "href": "posts/2021-06-25-how-many-young-mormons-are-queer/index.html#footnotes",
    "title": "Re-weighting to estimate population characteristics within a subset of a survey sample",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ve used RMSE for validating predictions before but I have no idea if this is a thing w.r.t. weighting schemes. Feel free to chime in. The point here was just to somehow describe the differences between those three methods in a single metric.↩︎\nFor Party ID, this is true both for the specific population target I selected and for how I calculated Party ID using Nationscape variables.↩︎\nOne other potentially important caveat is that this Nationscape sample may overestimate the prevalence of different sexual identities across the board. One data point for that hypothesis is that Nationscape LGB estimates by generation were uniformly about 2 percentage points higher than what Gallup found for each generation in response to the same question earlier this year.↩︎\nThere certainly are issues with representativeness there, though—68% of respondents, for example, were female (likely inflating the estimate for queer students, since it appears more women are bisexual than men).↩︎"
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "",
    "text": "Last month, I made a Twitter bot using RStudio. Every few hours, the bot posts a satellite image and a Wikipedia reference for a given pair of coordinates from my home state of Utah. The bot is inspired by @mattdray’s @londonmapbot."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#what",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#what",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "",
    "text": "Last month, I made a Twitter bot using RStudio. Every few hours, the bot posts a satellite image and a Wikipedia reference for a given pair of coordinates from my home state of Utah. The bot is inspired by @mattdray’s @londonmapbot."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#how",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#how",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "How",
    "text": "How\n\nI started by forking Matt’s repository and following his instructions here.\nI made the following changes to his workflow:\n\nInstead of sampling from the greater London area, I sample from a rectangle that contains Utah state boundaries.\nI found a .shp file from gis.utah.gov and use that file and the sf package to test if the coordinate pair is inside or outside state boundaries. If necessary, I draw new pairs until I find one within state boundaries.\nI request larger (1280x1280) images from Mapbox.\nWikipedia has a feature that shows all pages within 10 km of a given point (such as your current location). Apparently, Wikipedia pages for any “location, structure, or geographic feature that is more or less fixed in one place” have latitude and longitude coordinates encoded. Who knew? It’s possible, then, to paste any coordinate pair into a URL and see what’s nearby. I do that, and add RSelenium functions to my script to return the text from the first hit on the resultant page. This text is appended to the body of the tweet as a “Nearby point of interest.”\nI add a hashtag to the body of the tweet to make the account a little more discoverable. The hashtag is randomly selected from a list of a dozen or so that I thought were relevant to the account’s content.\nI use @hadleywickham’s emo package to add emoji to brighten up the final result."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#why",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#why",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "Why",
    "text": "Why\n\nI wanted to learn more about Github Actions. It’s a surprisingly powerful feature; I definitely will be experimenting more.\nI’ve been feeling like I should focus on live projects. I’ve done a fair number of personal projects in the last year, but nearly all of them are static and live on my hard drive. This felt like an easy way to get something more dynamic off the ground.\nI’m still interested in expanding the project by adding an image classification component. When I first started, I was struck by how many of the satellite images showed completely empty space, and I thought it would be interesting to quantify just how many of them are (say, “contain buildings/roads/farms” vs “don’t contain”). I ran out of time and energy for now for this portion, but this kind of satellite data still feels like a good canvas for demonstrating that kind of model. I might come back to it around the holidays when I have a little more time.\nI think Twitter is fun."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#questions",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#questions",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "Questions",
    "text": "Questions\n\nMy repository’s action has started to fail intermittently—looking through the run history, it probably only succeeded a third of the time this week. Why? It looks like an issue with the webscraping component (I’m seeing a lot of “HTTP Error 303”), but I can’t figure out why this would happen at some times and not others."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#some-of-my-favorite-recent-textures",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#some-of-my-favorite-recent-textures",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "Some of my favorite recent textures",
    "text": "Some of my favorite recent textures\nNote: recent changes to the Twitter API seem to have caused the images to no longer display inline."
  },
  {
    "objectID": "posts/2022-11-27-writing-manifoldr-wrapper/index.html",
    "href": "posts/2022-11-27-writing-manifoldr-wrapper/index.html",
    "title": "Writing an R wrapper for a prediction market’s API",
    "section": "",
    "text": "I recently wrote an R wrapper for an online prediction market’s API.\n\nWhy?\n\nAfter stumbling upon the platform, I was interested in finding an easier way to exploit potential market inefficiencies programmatically\nIt had been a while since I’d written code for an R package and I wanted to refresh my memory\n\n\n\nManifold Markets\nThe prediction market is called Manifold Markets. Manifold has been described as a “play-money prediction market platform where you can bet on anything,” an “experiment for enabling effective forecasters to direct altruistic donations,” and “like Wikipedia for things that nobody knows yet but will be observable later.” It’s something like PredictIt without real money. The platform is still pretty new and the community is still pretty small, but it’s worth checking out.\n\n\n\nMy contribution\nThe wrapper is called manifoldr. Building the wrapper involved mapping the API’s HTTP-based interface into R functions. It required careful parsing of the API’s responses and ensuring that each function performed its corresponding API request effectively. This involved considerable error checking and handling to ensure robustness. It provides a fairly straightforward way to make API calls to Manifold via R functions. The main package function is manifold_api(), from which all of the API endpoints can be accessed successfully as of November 2022.\nFor example, we can retrieve user information by their unique username (in this case, the official account @ManifoldMarkets).\n\n# devtools::install_github(\"jcblsn/manifoldr\")\n\nmanifoldr::manifold_api(\n  endpoint = \"/v0/user/ManifoldMarkets\", \n  request_type = \"GET\"\n)\n\nA number of convenience functions are also provided. These include functions which correspond to specific endpoints along with others such as clean_manifold_content(), which will return output as a data frame with clean variable names. Users can also authenticate with the platform using manifoldr::get_manifold_api_key().\n\nmanifoldr::get_market(market_id_or_slug = \"will-the-los-angeles-lakers-make-th-8cbc520d8ca6\") |&gt; \n  manifoldr::clean_manifold_content()\n\nThe package includes implementations of standard unit testing and code coverage tools using covr, testthat, and Github Actions.\n\n\nIllustration\nTo demonstrate the package tools, I made a new account on the platform called “Manifold NBA” and programmatically set up prediction markets for all 30 American professional basketball teams’ playoff odds. Feel free to check those out here.\n\n\n\nFeedback\nThe API is still in alpha, so I haven’t built out convenience functions for every endpoint yet. I do plan to continue maintaining and updating the package though, so if you have any suggestions or feedback, please let me know in the comment section below or by opening up an issue inside the package repository.\n\n\nResources\nFinally, I’ve made a short list of resources that were helpful to me while I worked on this.\n\nA vignette on “Best practices for API packages” found in the httr package documentation\nHadley Wickham and Jenny Bryan’s comprehensive “R Packages”\nAnother vignette from httr on secret management, which was necessary in order to implement unit testing for endpoints that require authentication"
  },
  {
    "objectID": "posts/2023-05-08-m1-tensorflow/index.html",
    "href": "posts/2023-05-08-m1-tensorflow/index.html",
    "title": "Setting up TensorFlow with GPU support on a M1 Macbook Pro",
    "section": "",
    "text": "As has been noted by many others, Google search results seem to have meaningfully degraded in quality in the last couple of years. Whether this is due to AI content farms or more widespread use of SEO techniques, I sometimes find myself searching through what feels like ever-larger haystacks for increasingly well-disguised needles.\nThis was especially true for me last fall after I upgraded my laptop to a M1 Macbook Pro. Setting up a Python environment for TensorFlow with GPU support was surprisingly hard to do despite no shortage of helpful-sounding search results.\nAfter getting things up and running and subsequently having helped a couple of my classmates walk through the same process on their computers, I thought I’d write down the steps that worked with my machine."
  },
  {
    "objectID": "posts/2023-05-08-m1-tensorflow/index.html#introduction",
    "href": "posts/2023-05-08-m1-tensorflow/index.html#introduction",
    "title": "Setting up TensorFlow with GPU support on a M1 Macbook Pro",
    "section": "",
    "text": "As has been noted by many others, Google search results seem to have meaningfully degraded in quality in the last couple of years. Whether this is due to AI content farms or more widespread use of SEO techniques, I sometimes find myself searching through what feels like ever-larger haystacks for increasingly well-disguised needles.\nThis was especially true for me last fall after I upgraded my laptop to a M1 Macbook Pro. Setting up a Python environment for TensorFlow with GPU support was surprisingly hard to do despite no shortage of helpful-sounding search results.\nAfter getting things up and running and subsequently having helped a couple of my classmates walk through the same process on their computers, I thought I’d write down the steps that worked with my machine."
  },
  {
    "objectID": "posts/2023-05-08-m1-tensorflow/index.html#why",
    "href": "posts/2023-05-08-m1-tensorflow/index.html#why",
    "title": "Setting up TensorFlow with GPU support on a M1 Macbook Pro",
    "section": "Why?",
    "text": "Why?\nTraining neural networks locally with GPU support is many times faster than training them with a CPU alone.\nFor me, this was nice to have last semester while I was taking a deep learning class. I prefer writing code in VS Code over other environments and GPU acceleration made it feasible for me to do most of my coursework there instead of in Google Colab."
  },
  {
    "objectID": "posts/2023-05-08-m1-tensorflow/index.html#steps",
    "href": "posts/2023-05-08-m1-tensorflow/index.html#steps",
    "title": "Setting up TensorFlow with GPU support on a M1 Macbook Pro",
    "section": "Steps",
    "text": "Steps\n\n1. Install miniforge or miniconda\nI had issues when I tried these steps using Anaconda. I used miniconda in the end.\n\n\n2. Create a osx-arm64-native Conda environment\nPer this Stack Overflow answer, we’ll create a new Conda environment with the osx-arm64 channel as the default.\nCONDA_SUBDIR=osx-arm64 conda create -n native numpy -c conda-forge\nconda activate native \nconda config --env --set subdir osx-arm64\nSubsequent steps come from “Fix #2” in the following answer.\n\n\n3. Designate the use of the conda-forge channel for package installation\nconda config --add channels conda-forge\nconda config --set channel_priority strict \n\n\n4. Install packages\nHere we will install the TensorFlow dependencies and TensorFlow itself with versions as shown below.\nconda install -y -c apple tensorflow-deps==2.10.0\npython -m pip install tensorflow-macos==2.10.0\npython -m pip install tensorflow-metal==0.6.0\n\n\n5. Verify proper installation\npython --version\nconda list|grep -E '(tensorflow|numpy)'\nThis portion should yield the following:\nPython 3.10.8\nnumpy                     1.23.2          py310h127c7cf_0    conda-forge\ntensorflow-deps           2.10.0                        0    apple\ntensorflow-estimator      2.10.0                   pypi_0    pypi\ntensorflow-macos          2.10.0                   pypi_0    pypi\ntensorflow-metal          0.6.0                    pypi_0    pypi\n\n\n6. Test\nAnd that should be it! After completing those steps, the following code should yield a list that includes both a CPU and a GPU.\n\nimport tensorflow as tf\ntf.config.list_physical_devices()\n\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
  },
  {
    "objectID": "posts/2023-11-06-goatcounter-with-quarto/index.html",
    "href": "posts/2023-11-06-goatcounter-with-quarto/index.html",
    "title": "Privacy-friendly web analytics for Quarto",
    "section": "",
    "text": "The following is mostly to pin this solution for my future self.\nI’ve set up Google Analytics on a personal site before but I’ve never bothered to do anything with the resultant data. I’m interested in knowing whether the site is receiving visitors but I’m also pretty sympathetic to high privacy standards. GoatCounter turned out to be a nice fit for this combination of preferences.\n\nI’ll let the GoatCounter site speak for itself if you haven’t heard of it, but the gist is that it’s a lightweight, privacy-respecting analytics service. It’s open source and free for small sites.\nAnd, it turns out, it’s also easy to configure with Quarto. After you create an account, you’re presented with a snippet of JavaScript to embed in your site:\n&lt;script data-goatcounter=\"https://[your-code].goatcounter.com/count\" \nasync src=\"//gc.zgo.at/count.js\"&gt;&lt;/script&gt;\nQuarto provides the option to fairly easily embed custom JavasSript in each of the site’s HTML documents. To add tracking functionality to my site, I included the Javascript in my project’s _quarto.yml file as follows:\n# ...\nformat:\n  html:\n    theme: \n    # ...\n    css: styles.css\n    include-in-header: \n      - text: |\n          &lt;script data-goatcounter=\"https://[your-code].goatcounter.com/count\" \n          async src=\"//gc.zgo.at/count.js\"&gt;&lt;/script&gt;\nFrom there, all that was left was re-rendering the site with quarto render, pushing my changes to GitHub to trigger a new Netlify build, and checking the GoatCounter dashboard to confirm that everything was working."
  },
  {
    "objectID": "posts/2023-11-06-goatcounter-for-quarto/index.html",
    "href": "posts/2023-11-06-goatcounter-for-quarto/index.html",
    "title": "Privacy-friendly web analytics for Quarto",
    "section": "",
    "text": "The following is mostly to pin this solution for my future self.\nI’ve set up Google Analytics on a personal site before but I’ve never bothered to do anything with the resultant data. I’m interested in knowing whether the site is receiving visitors but I’m also pretty sympathetic to high privacy standards. GoatCounter turned out to be a nice fit for this combination of preferences.\n\nI’ll let the GoatCounter site speak for itself if you haven’t heard of it, but the gist is that it’s a lightweight, privacy-respecting analytics service. It’s open source and free for small sites.\nAnd, it turns out, it’s also easy to configure with Quarto. After you create an account, you’re presented with a snippet of JavaScript to embed in your site:\n&lt;script data-goatcounter=\"https://[your-code].goatcounter.com/count\" \nasync src=\"//gc.zgo.at/count.js\"&gt;&lt;/script&gt;\nQuarto provides the option to fairly easily embed custom JavasSript in each of the site’s HTML documents. To add tracking functionality to my site, I included the Javascript in my project’s _quarto.yml file as follows:\n# ...\nformat:\n  html:\n    theme: \n    # ...\n    css: styles.css\n    include-in-header: \n      - text: |\n          &lt;script data-goatcounter=\"https://[your-code].goatcounter.com/count\" \n          async src=\"//gc.zgo.at/count.js\"&gt;&lt;/script&gt;\nFrom there, all that was left was re-rendering the site with quarto render, pushing my changes to GitHub to trigger a new Netlify build, and checking the GoatCounter dashboard to confirm that everything was working."
  },
  {
    "objectID": "posts/2023-11-11-server-configuration/index.html",
    "href": "posts/2023-11-11-server-configuration/index.html",
    "title": "Documenting my home server setup",
    "section": "",
    "text": "You may or may not want to set up your own server at home.\nI did, mostly for learning purposes and for data science projects. I don’t have a lot of experience with networking and there’s nothing like learning by doing. The following is a description of the process I followed for future reference.\nI bought a HP ProDesk 600 G5 Mini (Intel i5-9500T, 16GB RAM, 512GB HDD) on eBay for ~$150. This seems like a reasonable price for a capable machine that’s small and energy efficient. There are obviously many options; I wanted something more powerful than a Raspberry Pi but less expensive than a workstation.\n\n\n\n\n\n\nNote\n\n\n\n\nBesides the server itself, I needed the following peripherals to get started:\n\nmonitor for initial setup\nkeyboard for initial setup\nEthernet cable\npower cable\nflash drive for Ubuntu installer"
  },
  {
    "objectID": "posts/2023-11-11-server-configuration/index.html#intro",
    "href": "posts/2023-11-11-server-configuration/index.html#intro",
    "title": "Documenting my home server setup",
    "section": "",
    "text": "You may or may not want to set up your own server at home.\nI did, mostly for learning purposes and for data science projects. I don’t have a lot of experience with networking and there’s nothing like learning by doing. The following is a description of the process I followed for future reference.\nI bought a HP ProDesk 600 G5 Mini (Intel i5-9500T, 16GB RAM, 512GB HDD) on eBay for ~$150. This seems like a reasonable price for a capable machine that’s small and energy efficient. There are obviously many options; I wanted something more powerful than a Raspberry Pi but less expensive than a workstation.\n\n\n\n\n\n\nNote\n\n\n\n\nBesides the server itself, I needed the following peripherals to get started:\n\nmonitor for initial setup\nkeyboard for initial setup\nEthernet cable\npower cable\nflash drive for Ubuntu installer"
  },
  {
    "objectID": "posts/2023-11-11-server-configuration/index.html#definitions",
    "href": "posts/2023-11-11-server-configuration/index.html#definitions",
    "title": "Documenting my home server setup",
    "section": "Definitions",
    "text": "Definitions\nThe following were some concepts I unfamiliar with or only somewhat familiar with before starting. This isn’t meant to be a comprehensive guide as much as it is a journal of what I personally learned (mostly from ChatGPT) along the way.\n\nNetworking\n\nDHCP (Dynamic Host Configuration Protocol): A server protocol that automatically assigns IP addresses to devices on a network, ensuring they can communicate effectively without address conflicts.\nIPv4 vs IPv6: These are versions of the Internet Protocol. IPv4 uses a 32-bit address space, while IPv6 uses 128 bits, allowing for a vast number of unique addresses.\nNameservers: These are the internet’s equivalent of a phone book, translating human-readable domain names to IP addresses.\nDNS (Domain Name System): This system translates domain names like www.example.com into IP addresses that network devices use to identify each other.\nSubnet: A subdivision of an IP network that can segment a large network into smaller, more manageable pieces.\nNameservers: Specialized servers that translate domain names into IP addresses, allowing browsers to load internet resources.\n\n\n\nHardware\n\nBIOS (Basic Input/Output System): Firmware that initializes hardware during the boot process and provides runtime services for the operating system.\n\n\n\nSoftware\n\nNTP (Network Time Protocol): A networking protocol for clock synchronization between computer systems over packet-switched, variable-latency data networks.\nLUKS (Linux Unified Key Setup): A disk encryption specification that provides a platform-independent standard on-disk-format for use in various tools.\nLVM (Logical Volume Manager): A device mapper target that provides logical volume management for the Linux kernel."
  },
  {
    "objectID": "posts/2023-11-11-server-configuration/index.html#process",
    "href": "posts/2023-11-11-server-configuration/index.html#process",
    "title": "Documenting my home server setup",
    "section": "Process",
    "text": "Process\n\nPurchase HP ProDesk 600 G5 Mini\nBoot Windows and make note of Windows product key\nUsing my MacBook and a flash drive, create a bootable Ubuntu 22.04.3 LTS installer:\n\ndiskutil list to list disks\ndiskutil unmountDisk /dev/disk[number corresponding to flash drive from output previous command] i.e. diskutil unmountDisk /dev/disk4 to unmount flash drive\nsudo dd if=/local/path/to/ubuntu-22.04.3-live-server-amd64.iso of=/dev/rdisk[number from above] bs=1m to copy iso to flash drive (be very careful to use the right disk number! also note ‘r’ in of path)\n\nUpdate BIOS settings on ProDesk to allow booting from USB:\n\nF10 (varies by machine) to enter BIOS settings\nMake sure USB Storage Boot is enabled and set to top of boot order\n\nPlug in flash drive to ProDesk and boot from USB:\n\nCycle power; if BIOS settings were reconfigured correctly, you should see a screen with options to try or install Ubuntu\n\nFollow prompts to install Ubuntu\n\nSelect “Try or Install Ubuntu Server”\nConfigure network connections\n\nSelect interface\nIPv4: “Manual”\nSubnet: Given in router settings\nAddress: Chosen based on router settings—find DHCP range and choose a number outside that range\nGateway: Given in router settings (usually same as address but with 1 as last number)\nName servers: Usually Google or Cloudflare’s, i.e. 1.1.1.1, 1.0.0.1\nSearch domains: “home”\n\nConfigure proxy:\n\nI skipped this to start with\n\nStorage configuration:\n\nUse entire disk\nLUKS encryption\nLVM\n\nCreate user\n\nChoose a username and password\n\nInstall OpenSSH server\nInstall updates\nReboot\n\nSet up firewall\n\nsudo ufw enable to enable firewall\nsudo ufw default deny incoming to block all incoming traffic\nsudo ufw default deny outgoing to block all outgoing traffic\nsudo ufw allow ssh to allow ssh\nsudo ufw allow [port number] to allow ssh from a safer port than the default\nsudo ufw allow out 80 to allow outgoing HTTP requests\nsudo ufw allow out 443 to allow outgoing HTTPS requests\nsudo ufw allow out 53 to allow outgoing DNS requests\nsudo ufw allow from any to any port 123 proto udp to allow NTP (for time synchronization)\nsudo ufw status verbose to check status\n\nConfigure ssh\n\nsudo apt update to update package lists\nsudo apt install openssh-server to make sure ssh server is installed (should have been already from installer)\nsudo systemctl status ssh to verify status\nsudo nano /etc/ssh/sshd_config to edit config file; add:\n\nPort [port number]\nPermitRootLogin no\nAllowUsers [username]\nPubkeyAuthentication yes\n\nsudo systemctl restart ssh to apply changes\nOn MacBook:\n\nssh-keygen -t rsa to generate key pair\n\nChoose default location\nEnter passphrase\n\nssh-copy-id -i ~/.ssh/id_rsa.pub -p [port number] [username]@[ip address] to copy public key to server\n\n\nConfigure fail2ban\n\nsudo apt install fail2ban\nsudo cp /etc/fail2ban/jail.{conf,local}\nsudo nano /etc/fail2ban/jail.local to add desired settings to config file\nsudo systemctl start fail2ban\nsudo systemctl enable fail2ban\nsudo fail2ban-client status sshd\nsudo tail -f /var/log/fail2ban.log to monitor logs\n\nSSH into live, configured server:\n\nssh -p [port number] [username]@[ip address]\nLook around to make sure everything looks good\n\nSet up server in long-term physical location\n\nUnplug server from monitor and plug into router via Ethernet\nPower on\nPlug in keyboard\nEnter LUKS password with spare keyboard (blindly)\nWait for a couple minutes\n\nSSH in from MacBook again"
  },
  {
    "objectID": "posts/2023-11-11-server-configuration/index.html#next-steps",
    "href": "posts/2023-11-11-server-configuration/index.html#next-steps",
    "title": "Documenting my home server setup",
    "section": "Next steps",
    "text": "Next steps\n\nInstall Fig, a command-line tool with lots of useful features like autocomplete and aliases\n\non MacBook: fig integrations install ss\non server: curl -fSsL https://repo.fig.io/scripts/install-headless.sh | bash\non server: exec bash --norc to reset shell\n\nSet timezone and sync (note: after a surprisingly long time fiddling with this, I punted on syncing and settled on setting the timezone using my location string)\nInstall glances for monitoring\n\nsudo apt install glances\nglances\n\nExpand volume size\n\nlvextend -L+156G /dev/mapper/ubuntu--vg-ubuntu--lv\nsudo resize2fs /dev/mapper/ubuntu--vg-ubuntu--lv"
  },
  {
    "objectID": "posts/2023-11-11-server-configuration/index.html#conclusion",
    "href": "posts/2023-11-11-server-configuration/index.html#conclusion",
    "title": "Documenting my home server setup",
    "section": "Conclusion",
    "text": "Conclusion\nThis is all fairly boilerplate stuff but I just want to hedge again that this post is primarily to keep track of what I did for future reference and not an authoritative guide. That said, if you see something in this post that you think departs from best practices, I would love to hear about it—please comment below or reach out. As I said above, this is an educational exercise and I’m keen to learn more about Linux in particular."
  }
]
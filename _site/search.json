[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Writing an R wrapper for a prediction market‚Äôs API\n\n\n\n\n\n\n\nR\n\n\nR Packages\n\n\nForecasting\n\n\n\n\nHow to ‚Äúbet on anything with play money‚Äù from your console\n\n\n\n\n\n\nNov 27, 2022\n\n\nJacob Eliason\n\n\n\n\n\n\n  \n\n\n\n\nDeploying a Twitter bot from RStudio\n\n\n\n\n\n\n\nR\n\n\nTwitter\n\n\nGithub Actions\n\n\n\n\nLast month, I made a Twitter bot using RStudio and Github Actions.\n\n\n\n\n\n\nNov 11, 2021\n\n\nJacob Eliason\n\n\n\n\n\n\n  \n\n\n\n\nPreparing data from Strava for analysis\n\n\n\n\n\n\n\nR\n\n\nSports\n\n\n\n\nA process description of how I clean data collected from my runs.\n\n\n\n\n\n\nApr 30, 2021\n\n\nJacob Eliason\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jacob Eliason",
    "section": "",
    "text": "I am a data scientist currently working towards a master‚Äôs degree in statistics at the London School of Economics.\nI have experience in statistical modeling, deep learning, and survey research. I like working on hard problems.\nI also enjoy running, hiking, and film photography."
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Jacob Eliason",
    "section": "",
    "text": "I am a data scientist currently working towards a master‚Äôs degree in statistics at the London School of Economics.\nI have experience in statistical modeling, deep learning, and survey research. I like working on hard problems.\nI also enjoy running, hiking, and film photography."
  },
  {
    "objectID": "posts/2021-04-30-processing-data-from-strava/index.html",
    "href": "posts/2021-04-30-processing-data-from-strava/index.html",
    "title": "Preparing data from Strava for analysis",
    "section": "",
    "text": "When I run, I use Strava to log my activity. In honor of recently running my one-thousandth mile on Strava, I thought I‚Äôd do a write up for the steps I use to process my user data in R. The data Strava makes available is granular and can be used for all kinds of fun things after the steps detailed here."
  },
  {
    "objectID": "posts/2021-04-30-processing-data-from-strava/index.html#s1",
    "href": "posts/2021-04-30-processing-data-from-strava/index.html#s1",
    "title": "Preparing data from Strava for analysis",
    "section": "1. Export your data",
    "text": "1. Export your data\nPer the instructions on their website, you can export your Strava activity data by navigating to your profile in a web browser and following Settings &gt; My Account &gt; Download or Delete your Account - Get Started &gt; Request Your Archive. From that point, it takes me about 10 minutes to see a download link in my inbox.\nThe download apparently includes a lot of different kinds of data but the most salient (for my account, anyway) are contained in activities.csv and the activities/ directory. The former contains summary information for each of my Strava activities and the latter contains individual files, each of which have second-to-second position data for an individual run, hike, or bike ride. The activity files appear to be some kind of custom or proprietary exercise file type‚Äìthe two extensions I notice are .gpx and .fit.gz. At first glance, I don‚Äôt recognize either.\nFortunately, as usual I find that someone else has already done the heavy lifting for the most important part of this process. The Github packages FITfileR and trackeR can be used to convert these file types into something more legible. Special thanks to Mike Smith for his excellent work on the former."
  },
  {
    "objectID": "posts/2021-04-30-processing-data-from-strava/index.html#s2",
    "href": "posts/2021-04-30-processing-data-from-strava/index.html#s2",
    "title": "Preparing data from Strava for analysis",
    "section": "2. Unpacking .gpx and .fit.gz files",
    "text": "2. Unpacking .gpx and .fit.gz files\nI start by installing the Github packages and loading those along with the tidyverse.\n\n# devtools::install_github(\"grimbough/FITfileR\")\n# devtools::install_github(\"trackerproject/trackeR\")\n\nlibrary(FITfileR)\nlibrary(trackeR)\nlibrary(tidyverse)\n\nA few more lines help with setup and prepare for reading the activity files.\n\nPATH &lt;- str_c(str_remove(getwd(),\"/jacobeliason.com/posts/2021-04-30-processing-data-from-strava\"),\"/personal-projects/strava\")\nexport_date &lt;- \"2021-04-29\"\nPATH_ACTIVITIES &lt;- str_c(PATH, \"/DATA/\",export_date,\"/activities/\")\nactivity_names &lt;- list.files(PATH_ACTIVITIES)\nsample(activity_names, 3) # check to make sure I got the correct file path\n\n[1] \"3162985846.gpx\" \"3366567990.fit\" \"3337555997.gpx\"\n\n\nAs I look at the file names, the first thing that becomes apparent is that I have some extra work to do as a result of my alternately using my phone and a Garmin watch to record activities. Those two devices produce the two different file extensions I observe and require different steps for unpacking.\n\nUncompressing and reading files from my fitness watch (.fit.gz)\nThe .fit.gz files are compressed and need to be uncompressed to .fit before I can use the FITfileR package.\n\ncompressed_record_names &lt;- activity_names[str_sub(activity_names,-6,-1) == \"fit.gz\"]\n\nfor(i in 1:length(compressed_record_names)){\n  R.utils::gunzip(\n    str_c(PATH_ACTIVITIES, compressed_record_names[i]),\n    remove = F\n  )\n}\n\nHaving unzipped the files, I again collect names.\n\nactivity_names &lt;- list.files(PATH_ACTIVITIES)\nuncompressed_fit_names &lt;- activity_names[str_sub(activity_names,-3,-1) == \"fit\"] # want exact match to .fit only, no .fit.gz\n\nNow, using FITfileR::records(), I transform the files into tidy, rectangular datasets.\n\nlist.fit &lt;- list()\nfor(i in 1:length(uncompressed_fit_names)) {\n  record &lt;- FITfileR::readFitFile(\n    str_c(PATH_ACTIVITIES, uncompressed_record_names[i])\n  ) %&gt;% FITfileR::records()\n  \n  if(length(record) &gt; 1) {\n    record &lt;- record %&gt;% bind_rows() %&gt;% mutate(activity_id = i, filename = uncompressed_fit_names[i])\n  }\n  \n  list.fit[[i]] &lt;- record\n}\nfit_records &lt;- list.fit %&gt;% bind_rows() %&gt;% arrange(timestamp)\n\n\n\nReading files recorded from my iPhone (.gpx)\nI turn my attention back to the .gpx files. Fortunately, these files don‚Äôt require much beyond a simple pass from the trackeR function. I do some additional housekeeping along the way, but this part is pretty straightforward.\n\ngpx_names &lt;- activity_names[str_sub(activity_names,-3,-1) == \"gpx\"]\n\nlist.gpx &lt;- list()\nfor(i in 1:length(gpx_names)) {\n  record &lt;- trackeR::readGPX(str_c(PATH_ACTIVITIES, gpx_names[i])) %&gt;% \n    as_tibble() %&gt;% \n    rename(\n      timestamp = time, \n      position_lat = latitude, \n      position_long = longitude, \n      cadence = cadence_running\n    )\n  list.gpx[[i]] &lt;- record\n}\n\n\n\nCombine both record types\nI add my two datasets together and with that, I‚Äôm ready to Learn Things.\n\nrecords &lt;- bind_rows(\n  fit_records,\n  list.gpx %&gt;% bind_rows()\n) %&gt;% arrange(timestamp)\n\n# colnames(records)\n# nrow(records)\n\n\n\nStraightening out the summary information in activities.csv\nOne last thing I‚Äôll do before I finish up is make some tweaks to the activities.csv file I got in my original download. There‚Äôs a surprising number of columns; it‚Äôs kind of unwieldy when you first open it up. Here‚Äôs a taste of what‚Äôs in there.\n\nactivities &lt;- read_csv(str_c(PATH, \"/DATA/\",export_date,\"/\",\"activities.csv\"))\n\ndim(activities)\n\n[1] 250  78\n\nactivities %&gt;% colnames() %&gt;% sample(., 10, F) \n\n [1] \"Other Time\"                \"Cloud Cover\"              \n [3] \"Elevation Gain\"            \"Activity Name\"            \n [5] \"Number of Runs\"            \"Humidity\"                 \n [7] \"Prefer Perceived Exertion\" \"Weather Observation Time\" \n [9] \"Distance...16\"             \"Average Negative Grade\"   \n\n\nI make some changes to the column names and order to taste, and I remove rows with empty file names. It turns out that those correspond with activities with no associated GPS data, such as treadmill or weightlifting workouts.\n\nrecord_key_raw &lt;- \n  activities %&gt;% \n  janitor::clean_names() %&gt;% # helper function for column names\n  janitor::remove_empty() %&gt;% # drop empty rows\n  select(filename, everything()) %&gt;% # reorder columns\n  filter(!is.na(filename)) # drop rows with empty file names\n\nI also make a variety of mostly trivial changes for my own convenience and then I‚Äôm good to go!\n\nKM_TO_MI &lt;- 0.621371\nM_TO_FT &lt;- 3.28084\n\nrecord_key &lt;- record_key_raw %&gt;% \n  \n  # change units for elevation variables\n  mutate_at(vars(contains(\"elevation\")), function(x){x &lt;- x*M_TO_FT}) %&gt;% \n  mutate(\n    \n  # units #\n    distance = distance*KM_TO_MI,\n    duration = elapsed_time/60,\n    duration_moving = moving_time/60,\n    pace = (duration/distance) %&gt;% round(2),\n    pace_moving = (duration_moving/distance) %&gt;% round(2),\n    \n  # ids #\n    filename = filename %&gt;% str_remove(., \"activities/\") %&gt;% str_replace(., \"fit.gz\", \"fit\"),\n    activity_id = as.character(activity_id),\n    activity_type = tolower(activity_type),\n    \n  # incorrectly coded activities #\n    activity_type = ifelse(filename == \"1812636545.gpx\", \"hike\", activity_type), \n    activity_type = ifelse(filename == \"3324264305.fit\", \"walk\", activity_type), \n    \n    \n  # dates #\n    rdatetime_utc = lubridate::as_datetime(activity_date, format = \"%b %d, %Y, %I:%M:%S %p\", tz = \"UTC\"),\n    rdatetime_et = lubridate::as_datetime(rdatetime_utc, tz = \"America/New_York\"),\n    rdate_et = lubridate::as_date(rdatetime_et), \n    \n    rday = lubridate::day(rdate_et),\n    rmonth = lubridate::month(rdate_et),\n    ryear = lubridate::year(rdate_et),\n    rhour_et = lubridate::hour(rdatetime_et),\n    rminute_et = lubridate::minute(rdatetime_et)\n\n  ) %&gt;% \n  select( # drop empty variables\n    -contains(\"weather\"), -contains(\"precipitation\"), -contains(\"wind\"),\n    -apparent_temperature, -sunrise_time, -sunset_time, -dewpoint, -humidity, -cloud_cover, -uv_index\n  ) %&gt;% \n  mutate_if(is.numeric, ~round(.x, 2)) # round numeric variables\n\nNow, for each run, I have information on granular location data and summary information in datasets records and record_key respectively. The interesting stuff pretty much all comes after this point, but I‚Äôll save that for another post."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "",
    "text": "Last month, I made a Twitter bot using RStudio. Every few hours, the bot posts a satellite image and a Wikipedia reference for a given pair of coordinates from my home state of Utah. The bot is inspired by @mattdray‚Äôs @londonmapbot."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#what",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#what",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "",
    "text": "Last month, I made a Twitter bot using RStudio. Every few hours, the bot posts a satellite image and a Wikipedia reference for a given pair of coordinates from my home state of Utah. The bot is inspired by @mattdray‚Äôs @londonmapbot."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#how",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#how",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "How",
    "text": "How\n\nI started by forking Matt‚Äôs repository and following his instructions here.\nI made the following changes to his workflow:\n\nInstead of sampling from the greater London area, I sample from a rectangle that contains Utah state boundaries.\nI found a .shp file from gis.utah.gov and use that file and the sf package to test if the coordinate pair is inside or outside state boundaries. If necessary, I draw new pairs until I find one within state boundaries.\nI request larger (1280x1280) images from Mapbox.\nWikipedia has a feature that shows all pages within 10 km of a given point (such as your current location). Apparently, Wikipedia pages for any ‚Äúlocation, structure, or geographic feature that is more or less fixed in one place‚Äù have latitude and longitude coordinates encoded. Who knew? It‚Äôs possible, then, to paste any coordinate pair into a URL and see what‚Äôs nearby. I do that, and add RSelenium functions to my script to return the text from the first hit on the resultant page. This text is appended to the body of the tweet as a ‚ÄúNearby point of interest.‚Äù\nI add a hashtag to the body of the tweet to make the account a little more discoverable. The hashtag is randomly selected from a list of a dozen or so that I thought were relevant to the account‚Äôs content.\nI use @hadleywickham‚Äôs emo package to add emoji to brighten up the final result."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#why",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#why",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "Why",
    "text": "Why\n\nI wanted to learn more about Github Actions. It‚Äôs a surprisingly powerful feature; I definitely will be experimenting more.\nI‚Äôve been feeling like I should focus on live projects. I‚Äôve done a fair number of personal projects in the last year, but nearly all of them are static and live on my hard drive. This felt like an easy way to get something more dynamic off the ground.\nI‚Äôm still interested in expanding the project by adding an image classification component. When I first started, I was struck by how many of the satellite images showed completely empty space, and I thought it would be interesting to quantify just how many of them are (say, ‚Äúcontain buildings/roads/farms‚Äù vs ‚Äúdon‚Äôt contain‚Äù). I ran out of time and energy for now for this portion, but this kind of satellite data still feels like a good canvas for demonstrating that kind of model. I might come back to it around the holidays when I have a little more time.\nTwitter is fun, simple as."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#questions",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#questions",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "Questions",
    "text": "Questions\n\nMy repository‚Äôs action has started to fail intermittently‚Äîlooking through the run history, it probably only succeeded a third of the time this week. Why? It looks like an issue with the webscraping component (I‚Äôm seeing a lot of ‚ÄúHTTP Error 303‚Äù), but I can‚Äôt figure out why this would happen at some times and not others."
  },
  {
    "objectID": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#some-of-my-favorite-recent-textures",
    "href": "posts/2021-11-11-setting-up-a-twitter-bot/index.html#some-of-my-favorite-recent-textures",
    "title": "Deploying a Twitter bot from RStudio",
    "section": "Some of my favorite recent textures",
    "text": "Some of my favorite recent textures\n\n\nüìç 40.1263, -113.3602‚ÑπÔ∏è Nearby point of interest: Granite Peak Installation - 7.4 km awayüîó https://t.co/ODI4FztxYaüó∫ https://t.co/REpGX3zh9r#lifeelevated pic.twitter.com/AsoAtB6B9R‚Äî Textures of Utah ü§ñ (@texturesofut) November 6, 2021\n\n\n\nüìç 37.7156, -113.5968‚ÑπÔ∏è Nearby point of interest: Beryl Junction, Utah - CDP in Utah, United States - 5.3 km awayüîó https://t.co/mNY9WmBO6xüó∫ https://t.co/nTrmgILMGx#landsat pic.twitter.com/vh0hNixHUC‚Äî Textures of Utah ü§ñ (@texturesofut) November 3, 2021\n\n\n\nüìç 37.1025, -111.5024‚ÑπÔ∏è Nearby point of interest: Lone Rock (Glen Canyon National Recreation Area) - 8.9 km awayüîó https://t.co/6SGKTMUyTzüó∫ https://t.co/nV93hvWAUc#wikipedia pic.twitter.com/QXNeXUc92d‚Äî Textures of Utah ü§ñ (@texturesofut) November 2, 2021\n\n\n\nüìç 40.0148, -111.6805‚ÑπÔ∏è Nearby point of interest: Elk Ridge, Utah - City in Utah, United States - 150 m awayüîó https://t.co/nOf3QawMd7üó∫ https://t.co/qsASwKWtsp#mapbox pic.twitter.com/sSPuDb9C1b‚Äî Textures of Utah ü§ñ (@texturesofut) October 24, 2021\n\n\n\nüìç 37.4306, -113.1841‚ÑπÔ∏è Nearby point of interest: Timber Top Mountain - 860 m awayüîó https://t.co/I8wCZNOXf5üó∫ https://t.co/i0i1jgtFMC#tidyverse pic.twitter.com/wbVGaEVv3B‚Äî Textures of Utah ü§ñ (@texturesofut) October 20, 2021"
  },
  {
    "objectID": "posts/2022-11-27-writing-manifoldr-wrapper/index.html",
    "href": "posts/2022-11-27-writing-manifoldr-wrapper/index.html",
    "title": "Writing an R wrapper for a prediction market‚Äôs API",
    "section": "",
    "text": "I recently wrote an (?) R wrapper for an online prediction market‚Äôs API.\n\nWhy?\n\nAfter stumbling upon the platform, I was interested in finding an easier way to exploit potential market inefficiencies programmatically\nIt had been a while since I‚Äôd written code for an R package and I wanted to refresh my memory\n\n\n\nThe platform\nThe prediction market is called Manifold Markets. Manifold has been described as a ‚Äúplay-money prediction market platform where you can bet on anything,‚Äù an ‚Äúexperiment for enabling effective forecasters to direct altruistic donations,‚Äù and ‚Äúlike Wikipedia for things that nobody knows yet but will be observable later.‚Äù It‚Äôs something like PredictIt without real money. The platform is still pretty new and the community is still pretty small, but it‚Äôs worth checking out.\n\n\n\nMy contribution\nThe wrapper is called manifoldr. It provides a fairly straightforward way to make API calls to Manifold via R functions. The main package function is manifold_api(), from which all of the API endpoints can be accessed successfully as of November 2022.\nFor example, we can retrieve user information by their unique username (in this case, the official account @ManifoldMarkets).\n\n# devtools::install_github(\"jcblsn/manifoldr\")\n\nmanifoldr::manifold_api(\n  endpoint = \"/v0/user/ManifoldMarkets\", \n  request_type = \"GET\"\n)\n\nA number of convenience functions are also provided. These include functions which correspond to specific endpoints along with others such as clean_manifold_content(), which will return output as a data frame with clean variable names. Users can also authenticate with the platform using manifoldr::get_manifold_api_key().\n\nmanifoldr::get_market(market_id_or_slug = \"will-the-los-angeles-lakers-make-th-8cbc520d8ca6\") |&gt; \n  manifoldr::clean_manifold_content()\n\nThe package includes implementations of standard unit testing and code coverage tools using covr, testthat, and Github Actions.\n\n\nIllustration\nTo demonstrate the package tools, I made a new account on the platform called ‚ÄúManifold NBA‚Äù and programmatically set up prediction markets for all 30 American professional basketball teams‚Äô playoff odds. Feel free to check those out here.\n\n\n\nFeedback\nThe API is still in alpha, so I haven‚Äôt built out convenience functions for every endpoint yet. I do plan to continue maintaining and updating the package though, so if you have any suggestions or feedback, please let me know in the comment section below or by opening up an issue inside the package repository.\n\n\nResources\nFinally, here is a short list of resources that were helpful to me while I worked on this:\n\nA vignette on ‚ÄúBest practices for API packages‚Äù found in the httr package documentation\nHadley Wickham and Jenny Bryan‚Äôs comprehensive ‚ÄúR Packages‚Äù\nAnother vignette from httr on secret management, which was necessary in order to implement unit testing for endpoints that require authentication"
  }
]